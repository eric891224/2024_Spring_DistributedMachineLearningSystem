# Federated Instruction Tuning on General Dataset
---
num_clients: 30 # total number of clients
num_rounds: 200

dataset:
    name: "FinGPT/fingpt-sentiment-train"

model:
    name: "meta-llama/Llama-2-7b-hf"
    quantization: 8 # 8 or 4 if you want to do quantization with BitsAndBytes
    gradient_checkpointing: True
    lora:
        peft_lora_r: 8
        peft_lora_alpha: 16

train:
    num_rounds: ${num_rounds}
    save_every_round: 5
    learning_rate_max: 5e-5
    learning_rate_min: 1e-6
    seq_length: 512
    training_arguments:
        output_dir: null # to be set by hydra
        learning_rate: null # to be set by the client
        per_device_train_batch_size: 16
        gradient_accumulation_steps: 1
        logging_steps: 10
        num_train_epochs: 10
        max_steps: 10
        report_to: null
        save_steps: 1000
        save_total_limit: 10
        gradient_checkpointing: ${model.gradient_checkpointing}
        lr_scheduler_type: "constant"

strategy:
    _target_: flwr.server.strategy.FedAvg
    fraction_fit: 0.16666666666666666 # sample 10% of clients (i.e. 2 per round)
    fraction_evaluate: 0.0 # no client evaluation

client_resources:
    num_cpus: 8
    num_gpus: 1.0
